{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fdc4a16e9f0a8a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T15:39:02.075343Z",
     "start_time": "2024-04-13T15:38:58.414251Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from scipy.stats import multivariate_normal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6f937d63d2fac0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### [Section 1 – Multivariate Statistics]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e082ea0c6ed8bb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1. [10 points] Generate a data set of two independent and identically distributed (i.i.d.) variables of N(0,1), of length 5,000. How do you impose a correlation of, say, 0.5, to the data set? You can code up your work in your preferred programming language(s) or you can work it out in Excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "409c970c28ca69f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T18:35:09.837911Z",
     "start_time": "2024-04-13T18:35:09.807491Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.44988415],\n",
       "       [0.44988415, 1.        ]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "x = np.random.normal(0, 1, 5000)\n",
    "y = np.random.normal(0, 1, 5000)\n",
    "independent_vars = np.vstack([x, y]).T\n",
    "\n",
    "corr = 0.5\n",
    "\n",
    "# Cholesky decomposition of the correlation matrix\n",
    "corr = np.array([[1, corr], [corr, 1]])\n",
    "L = np.linalg.cholesky(corr)\n",
    "\n",
    "# Generating the correlated y_new\n",
    "correlated_vars = independent_vars @ L\n",
    "correlated_x, correlated_y = correlated_vars[:, 0], correlated_vars[:, 1]\n",
    "\n",
    "corr_new = np.corrcoef(correlated_x, correlated_y)\n",
    "corr_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79436118a6aa13eb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "2. [20 points] Following the above result, how would you apply a correlation of 0.5 to an empirical data set of two variables that start with a different correlation. That is, without modifying the marginal distributions of the two variables, convert the correlation of the data set to 0.5. Download S&P 500 index and USD/CAD FX rate historical data from 2019-12-31 to 202312-31 and apply your method to the historical data set. Yahoo Finance or other alternative data sources are all acceptable. You can code up your work in your preferred programming language(s) or work it out in Excel. Either way, you are also expected to explain your work in writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be1b47e7e971e7e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T15:39:14.664237Z",
     "start_time": "2024-04-13T15:39:13.060055Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "ASSET_LIST = ['US10Y_%', 'CA10Y_%', 'SPX_US$', 'TSX_CAD$', 'SPGSCI_USD$', 'Gold_USD$', 'USDCAD_CAD$']\n",
    "DF_LIST = []\n",
    "for asset in ASSET_LIST:\n",
    "    df = pd.read_excel('Data.xlsx', sheet_name=asset)\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df[asset] = df['close']\n",
    "    df.set_index('time', inplace=True)\n",
    "    DF_LIST.append(df[[asset]])\n",
    "\n",
    "DATA = pd.concat(DF_LIST, axis=1).dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47dfa3b6d8558282",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T15:39:16.901218Z",
     "start_time": "2024-04-13T15:39:16.858924Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>US10Y_%</th>\n",
       "      <th>CA10Y_%</th>\n",
       "      <th>SPX_US$</th>\n",
       "      <th>TSX_CAD$</th>\n",
       "      <th>SPGSCI_USD$</th>\n",
       "      <th>Gold_USD$</th>\n",
       "      <th>USDCAD_CAD$</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5457.00</td>\n",
       "      <td>5457.00</td>\n",
       "      <td>5457.00</td>\n",
       "      <td>5457.00</td>\n",
       "      <td>5457.00</td>\n",
       "      <td>5457.00</td>\n",
       "      <td>5457.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.23</td>\n",
       "      <td>3.07</td>\n",
       "      <td>2080.74</td>\n",
       "      <td>13361.13</td>\n",
       "      <td>446.00</td>\n",
       "      <td>1110.77</td>\n",
       "      <td>1.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.35</td>\n",
       "      <td>1.49</td>\n",
       "      <td>1120.90</td>\n",
       "      <td>4038.62</td>\n",
       "      <td>159.30</td>\n",
       "      <td>553.72</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.44</td>\n",
       "      <td>682.55</td>\n",
       "      <td>5695.33</td>\n",
       "      <td>130.29</td>\n",
       "      <td>252.10</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.15</td>\n",
       "      <td>1.78</td>\n",
       "      <td>1229.13</td>\n",
       "      <td>10485.20</td>\n",
       "      <td>337.76</td>\n",
       "      <td>560.10</td>\n",
       "      <td>1.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.03</td>\n",
       "      <td>2.94</td>\n",
       "      <td>1530.95</td>\n",
       "      <td>13407.01</td>\n",
       "      <td>437.54</td>\n",
       "      <td>1225.90</td>\n",
       "      <td>1.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.26</td>\n",
       "      <td>4.22</td>\n",
       "      <td>2743.07</td>\n",
       "      <td>15730.79</td>\n",
       "      <td>579.93</td>\n",
       "      <td>1557.90</td>\n",
       "      <td>1.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.79</td>\n",
       "      <td>6.60</td>\n",
       "      <td>5254.34</td>\n",
       "      <td>22361.78</td>\n",
       "      <td>890.29</td>\n",
       "      <td>2371.61</td>\n",
       "      <td>1.61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       US10Y_%  CA10Y_%  SPX_US$  TSX_CAD$  SPGSCI_USD$  Gold_USD$  \\\n",
       "count  5457.00  5457.00  5457.00   5457.00      5457.00    5457.00   \n",
       "mean      3.23     3.07  2080.74  13361.13       446.00    1110.77   \n",
       "std       1.35     1.49  1120.90   4038.62       159.30     553.72   \n",
       "min       0.51     0.44   682.55   5695.33       130.29     252.10   \n",
       "25%       2.15     1.78  1229.13  10485.20       337.76     560.10   \n",
       "50%       3.03     2.94  1530.95  13407.01       437.54    1225.90   \n",
       "75%       4.26     4.22  2743.07  15730.79       579.93    1557.90   \n",
       "max       6.79     6.60  5254.34  22361.78       890.29    2371.61   \n",
       "\n",
       "       USDCAD_CAD$  \n",
       "count      5457.00  \n",
       "mean          1.25  \n",
       "std           0.17  \n",
       "min           0.92  \n",
       "25%           1.11  \n",
       "50%           1.28  \n",
       "75%           1.35  \n",
       "max           1.61  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "afd59e72d75511ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T18:40:31.918862Z",
     "start_time": "2024-04-13T18:40:31.855067Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_Q2 = DATA[(DATA.index >= '2019-12-31') & (DATA.index <= '2023-12-31')][['SPX_US$', 'USDCAD_CAD$']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7f0f04c627627899",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T18:50:10.685747Z",
     "start_time": "2024-04-13T18:50:10.643486Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Correlation: -0.5019001388081297\n",
      "New Correlation: 0.5077164775034696\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Step 1: Transform to uniform using ECDF\n",
    "X = np.array(DATA_Q2['SPX_US$'])\n",
    "Y = np.array(DATA_Q2['USDCAD_CAD$'])\n",
    "quantile_transformer = QuantileTransformer(output_distribution='uniform', random_state=0, n_quantiles=len(X))\n",
    "U, V = quantile_transformer.fit_transform(np.column_stack((X, Y))).T\n",
    "\n",
    "# Step 2: Apply a Gaussian copula with the desired correlation\n",
    "desired_corr = np.array([[1, 0.5], [0.5, 1]])\n",
    "mvn = multivariate_normal(mean=[0, 0], cov=desired_corr)\n",
    "copula_samples = mvn.rvs(size=1000)\n",
    "new_U = norm.cdf(copula_samples[:, 0])\n",
    "new_V = norm.cdf(copula_samples[:, 1])\n",
    "\n",
    "# Step 3: Transform back to original scales\n",
    "X_new, Y_new = quantile_transformer.inverse_transform(np.column_stack((new_U, new_V))).T\n",
    "\n",
    "new_corr = np.corrcoef(X_new, Y_new)\n",
    "print(\"Original Correlation:\", np.corrcoef(X, Y)[0, 1])\n",
    "print(\"New Correlation:\", new_corr[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fd3ed95101be05",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "3. [10 points] How would you extend the above bivariate framework to multivariate distributions? Show your work in code or Excel and you can work with a set of 4 or 5 variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "461fc3e57b97a5c1",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T18:53:37.544800Z",
     "start_time": "2024-04-13T18:53:37.493124Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1.        , 0.49875481, 0.47121812, 0.49098476, 0.45255403],\n       [0.49875481, 1.        , 0.51287709, 0.53309411, 0.46951191],\n       [0.47121812, 0.51287709, 1.        , 0.47587352, 0.45413942],\n       [0.49098476, 0.53309411, 0.47587352, 1.        , 0.4680466 ],\n       [0.45255403, 0.46951191, 0.45413942, 0.4680466 , 1.        ]])"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_Q3 = DATA[(DATA.index >= '2019-12-31') & (DATA.index <= '2023-12-31')][\n",
    "    ['SPX_US$', 'USDCAD_CAD$', 'US10Y_%', 'SPGSCI_USD$', 'Gold_USD$']]\n",
    "DATA_Q3 = DATA_Q3.to_numpy()\n",
    "# Step 1: Transform marginals to uniform\n",
    "quantile_transformer = QuantileTransformer(output_distribution='uniform', random_state=0, n_quantiles=len(DATA_Q3))\n",
    "uniform_data = quantile_transformer.fit_transform(DATA_Q3)\n",
    "\n",
    "# Step 2: Create the desired correlation matrix\n",
    "desired_corr = np.full((5, 5), 0.5)\n",
    "np.fill_diagonal(desired_corr, 1)\n",
    "\n",
    "# Step 3: Apply Gaussian Copula\n",
    "mvn = multivariate_normal(mean=np.zeros(5), cov=desired_corr)\n",
    "copula_samples = mvn.rvs(size=1000)\n",
    "\n",
    "# Step 4: Convert copula samples to uniform\n",
    "uniform_samples = norm.cdf(copula_samples)\n",
    "\n",
    "# Step 5: Transform back to original scales\n",
    "transformed_data = quantile_transformer.inverse_transform(uniform_samples)\n",
    "\n",
    "# Step 6: Check the correlation of the transformed data\n",
    "transformed_corr = np.corrcoef(transformed_data.T)\n",
    "transformed_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcae5425894090c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### [Section 2 – Investment/Total Fund]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796a91834ae3cd8b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "4. [30 points] Imagine OTPP’s CIO and the Investment Committee decided to increase tactical allocation to the S&P 500 Index by CAD$1 billion, how would you execute the allocation and what implications would your execution decision have on the total fund besides the obvious impact on the equity asset class? Any other considerations should be examined? Explore the execution alternatives as many as possible and explain the impact to the total fund from asset risk and liquidity risk perspectives. The intention is to explore basic understanding of ﬁnancial products (cash vs derivative) and its implications in the context of asset allocation in a portfolio such as OTPP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad023fbd61c44e09",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "How would you execute the allocation and what implications would your execution decision have on the total fund besides the obvious impact on the equity asset class?\n",
    "\n",
    "My execution decision would be to buy S&P 500 futures contracts.\n",
    "\n",
    "Implications on the total fund:\n",
    "- Future is mark-to-market. It generates daily cashflow and impact liquidity.\n",
    "- Counterparty credit risk but minimal due to mark-to-market\n",
    "- Less capital required.\n",
    "- Need to roll\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba6bfe9324b662d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Explore the execution alternatives as many as possible and explain the impact to the total fund from asset risk and liquidity risk perspectives.\n",
    "\n",
    "Alternative 1: Buy S&P 500 ETFs\n",
    "Implications on the total fund:\n",
    "- Large trades in ETFs required slow execution to avoid moving the market\n",
    "- Less responsive because cannt liquidate quickly.\n",
    "- Much higher capital required.\n",
    "\n",
    "Alternative 2: Synthetic Long using S&P 500 Index Options (buy call option and sell put option)\n",
    "Implications on the total fund:\n",
    "- This approach takes volatility risk in addition to price risk\n",
    "- Cash flow impact whenever the options roll over or expire.\n",
    "- Dividend is not captured in the synthetic long position.\n",
    "- Less capital required.\n",
    "\n",
    "Alternative 3: Buy S&P 500 Index Total Return Swaps\n",
    "Implications on the total fund:\n",
    "- Leveraged exposure to the index with a smaller amount of capital.\n",
    "- counterparty risk. Mitigated by collateral posting and daily mark-to-market.\n",
    "- Liquidity risk due to OTC nature of the swaps. \n",
    "- Cash flow impact whenever the swaps roll over or expire.\n",
    "- upfront margin required to post as collateral.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe1b70058170b42",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### [Section 3 – Data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58799f18eccbef3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "5. [10 points] You are given two sets of time series. The lengths are slightly different and the dates are mostly the same but not exact. Below is an example. Can you come up with a strategy to align them so that the two time series are lined up for dates that are common to both sets, i.e., the intersection of the two date sets? You can choose to solve/guess the missing values in writing or in Excel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddf8155275945d9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The strategy is to join the two datasets on the 'Date' column, keeping only the rows where the date is common to both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d70b2aa9d27a496a",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T19:01:51.285492Z",
     "start_time": "2024-04-13T19:01:51.276811Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "         Date    SP500   USDCAD\n0  12/31/2019  3230.78  1.30606\n1  01/02/2020  3257.85  1.29730\n2  01/03/2020  3234.85  1.29830\n3  01/06/2020  3246.28  1.29866",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>SP500</th>\n      <th>USDCAD</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>12/31/2019</td>\n      <td>3230.78</td>\n      <td>1.30606</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>01/02/2020</td>\n      <td>3257.85</td>\n      <td>1.29730</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>01/03/2020</td>\n      <td>3234.85</td>\n      <td>1.29830</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>01/06/2020</td>\n      <td>3246.28</td>\n      <td>1.29866</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare Dataset\n",
    "df1 = pd.DataFrame(columns=['Date', 'SP500'])\n",
    "df2 = pd.DataFrame(columns=['Date', 'USDCAD'])\n",
    "\n",
    "df1.loc[len(df1)] = {'Date': '12/31/2019', 'SP500': 3230.78}\n",
    "df1.loc[len(df1)] = {'Date': '01/02/2020', 'SP500': 3257.85}\n",
    "df1.loc[len(df1)] = {'Date': '01/03/2020', 'SP500': 3234.85}\n",
    "df1.loc[len(df1)] = {'Date': '01/06/2020', 'SP500': 3246.28}\n",
    "\n",
    "df2.loc[len(df2)] = {'Date': '12/31/2019', 'USDCAD': 1.30606}\n",
    "df2.loc[len(df2)] = {'Date': '01/01/2020', 'USDCAD': 1.3002}\n",
    "df2.loc[len(df2)] = {'Date': '01/02/2020', 'USDCAD': 1.2973}\n",
    "df2.loc[len(df2)] = {'Date': '01/03/2020', 'USDCAD': 1.2983}\n",
    "df2.loc[len(df2)] = {'Date': '01/06/2020', 'USDCAD': 1.29866}\n",
    "\n",
    "df_merge = df1.merge(df2, how='inner', on='Date')\n",
    "df_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735072a19d834269",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Since the missing data is SP500 on 01/01/2020 and market is closed on 01/01/2020, we can use the last available data point to fill in the missing value. The implication is that the return on 01/01/2020 will be 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f69097d517de1d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "6. [10 points] Below is a table storing the Market Value (MV), DV01 and DV01 Convexity for a Fixed Income portfolio, FICA. However, there are a few missing values for the DV01 Convexity column. How would come up with an estimation for the missing DV01 Convexity values? How do you validate your method? You are welcome to try multiple methods for bonus points. Describe your thought process and the intent here is to test “number literacy” under a business context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "844f1ae936e95f04",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T19:04:31.772473Z",
     "start_time": "2024-04-13T19:04:31.711593Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "         Date Portfolio         MV    DV01  DV01Convexity\n0  10/27/2023      FICA   67454088  158800            NaN\n1  10/27/2023      FICA   89304605  230212            NaN\n2  10/27/2023      FICA  113440499  252102            NaN\n4  10/27/2023      FICA  182724762  398461         1029.0\n5  10/27/2023      FICA  182731405  398644         1029.0\n6  10/27/2023      FICA  183099003  399745         1032.0\n3  10/27/2023      FICA  185055847  404225         1042.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>Portfolio</th>\n      <th>MV</th>\n      <th>DV01</th>\n      <th>DV01Convexity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10/27/2023</td>\n      <td>FICA</td>\n      <td>67454088</td>\n      <td>158800</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10/27/2023</td>\n      <td>FICA</td>\n      <td>89304605</td>\n      <td>230212</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10/27/2023</td>\n      <td>FICA</td>\n      <td>113440499</td>\n      <td>252102</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10/27/2023</td>\n      <td>FICA</td>\n      <td>182724762</td>\n      <td>398461</td>\n      <td>1029.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>10/27/2023</td>\n      <td>FICA</td>\n      <td>182731405</td>\n      <td>398644</td>\n      <td>1029.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>10/27/2023</td>\n      <td>FICA</td>\n      <td>183099003</td>\n      <td>399745</td>\n      <td>1032.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10/27/2023</td>\n      <td>FICA</td>\n      <td>185055847</td>\n      <td>404225</td>\n      <td>1042.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare Dataset\n",
    "df_FI = pd.DataFrame(columns=['Date', 'Portfolio', 'MV', 'DV01', 'DV01Convexity'])\n",
    "df_FI.loc[len(df_FI)] = {'Date': '10/27/2023', 'Portfolio': 'FICA', 'MV': 67454088, 'DV01': 158800}\n",
    "df_FI.loc[len(df_FI)] = {'Date': '10/27/2023', 'Portfolio': 'FICA', 'MV': 89304605, 'DV01': 230212}\n",
    "df_FI.loc[len(df_FI)] = {'Date': '10/27/2023', 'Portfolio': 'FICA', 'MV': 113440499, 'DV01': 252102}\n",
    "df_FI.loc[len(df_FI)] = {'Date': '10/27/2023', 'Portfolio': 'FICA', 'MV': 185055847, 'DV01': 404225,\n",
    "                         'DV01Convexity': 1042}\n",
    "df_FI.loc[len(df_FI)] = {'Date': '10/27/2023', 'Portfolio': 'FICA', 'MV': 182724762, 'DV01': 398461,\n",
    "                         'DV01Convexity': 1029}\n",
    "df_FI.loc[len(df_FI)] = {'Date': '10/27/2023', 'Portfolio': 'FICA', 'MV': 182731405, 'DV01': 398644,\n",
    "                         'DV01Convexity': 1029}\n",
    "df_FI.loc[len(df_FI)] = {'Date': '10/27/2023', 'Portfolio': 'FICA', 'MV': 183099003, 'DV01': 399745,\n",
    "                         'DV01Convexity': 1032}\n",
    "df_FI.sort_values(by=['MV'], inplace=True)\n",
    "df_FI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "data": {
      "text/plain": "array([492.96554092, 661.39280282, 700.80135567])"
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Sample data with DV01 Convexity\n",
    "dfDataforModel = df_FI.dropna()\n",
    "\n",
    "LR = LinearRegression()\n",
    "X = dfDataforModel[['MV', 'DV01']]\n",
    "y = dfDataforModel['DV01Convexity']\n",
    "\n",
    "LR.fit(X, y)\n",
    "\n",
    "# Sample data missing dv01 Convexity\n",
    "dfDataforPrediction = df_FI[df_FI['DV01Convexity'].isna()]\n",
    "dfDataforPrediction = dfDataforPrediction[['MV', 'DV01']]\n",
    "\n",
    "# Predict missing DV01Convexity\n",
    "dfDataforPrediction = LR.predict(dfDataforPrediction)\n",
    "\n",
    "dfDataforPrediction"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T19:47:51.232871Z",
     "start_time": "2024-04-13T19:47:51.197592Z"
    }
   },
   "id": "62945ed1a00b5b79"
  },
  {
   "cell_type": "markdown",
   "id": "af4494049769e90d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "7. [15 points] What is data normalization in database design? Why and when do you normalize data? Make sure you include an example to illustrate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd4c11ab23e1015",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "What is data normalization in database design?\n",
    "reduce redundancy and improve data integrity by organizing fields and table of a database. \n",
    "\n",
    "Why Normalize Data?\n",
    "Avoids duplicate information. Saving storage space and ensuring that changes to data are reflected consistently across the database.\n",
    "Optimized data structures reduce the complexity of database operations and can improve the performance of the system.\n",
    "\n",
    "When to Normalize Data\n",
    "Normalization is typically beneficial in the following scenarios:\n",
    "Dataset is structured: When the data is organized into tables with rows and columns, normalization can help in reducing redundancy and improving data integrity.\n",
    "Data is subject to frequent updates.\n",
    "Data is shared across multiple tables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "633149314f841ec0",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T19:48:10.245504Z",
     "start_time": "2024-04-13T19:48:10.228013Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   OrderID CustomerName Product  Quantity  Price\n0        1        Alice   Apple         5   1.00\n1        2          Bob  Banana        10   0.50\n2        3        Alice  Cherry        15   2.00\n3        4      Charlie    Date        20   0.25",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>OrderID</th>\n      <th>CustomerName</th>\n      <th>Product</th>\n      <th>Quantity</th>\n      <th>Price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Alice</td>\n      <td>Apple</td>\n      <td>5</td>\n      <td>1.00</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>Bob</td>\n      <td>Banana</td>\n      <td>10</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>Alice</td>\n      <td>Cherry</td>\n      <td>15</td>\n      <td>2.00</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>Charlie</td>\n      <td>Date</td>\n      <td>20</td>\n      <td>0.25</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "data = {\n",
    "    'OrderID': [1, 2, 3, 4],\n",
    "    'CustomerName': ['Alice', 'Bob', 'Alice', 'Charlie'],\n",
    "    'Product': ['Apple', 'Banana', 'Cherry', 'Date'],\n",
    "    'Quantity': [5, 10, 15, 20],\n",
    "    'Price': [1.0, 0.5, 2.0, 0.25]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e46d6c49a55fe933",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T19:09:45.809230Z",
     "start_time": "2024-04-13T19:09:45.774817Z"
    }
   },
   "outputs": [],
   "source": [
    "# Normalized Tables\n",
    "df_customers = pd.DataFrame({'CustomerID': [1, 2, 3], 'CustomerName': ['Alice', 'Bob', 'Charlie']})\n",
    "df_products = pd.DataFrame({'ProductID': [1, 2, 3, 4], 'Product': ['Apple', 'Banana', 'Cherry', 'Date']})\n",
    "df_orders = pd.DataFrame(\n",
    "    {'OrderID': [1, 2, 3, 4], 'CustomerID': [1, 2, 1, 3], 'ProductID': [1, 2, 3, 4], 'Quantity': [5, 10, 15, 20],\n",
    "     'Price': [1.0, 0.5, 2.0, 0.25]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29aea9d2791c8a1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Customer Table (df_customers) with CustomerID as the primary key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b4bfb396f1f67c22",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T19:09:53.369031Z",
     "start_time": "2024-04-13T19:09:53.353349Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   CustomerID CustomerName\n0           1        Alice\n1           2          Bob\n2           3      Charlie",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CustomerID</th>\n      <th>CustomerName</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Alice</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>Bob</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>Charlie</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_customers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed05f6774f56c89",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Product Table (df_products) with ProductID as the primary key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a08fd6426f0d5793",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T19:09:54.917070Z",
     "start_time": "2024-04-13T19:09:54.897845Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   ProductID Product\n0          1   Apple\n1          2  Banana\n2          3  Cherry\n3          4    Date",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ProductID</th>\n      <th>Product</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Apple</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>Banana</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>Cherry</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>Date</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf41381beba805a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Orders Table (df_orders) with OrderID as the primary key and CustomerID and ProductID as foreign keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2f4349ca70bb77ca",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T19:09:57.681268Z",
     "start_time": "2024-04-13T19:09:57.656318Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   OrderID  CustomerID  ProductID  Quantity  Price\n0        1           1          1         5   1.00\n1        2           2          2        10   0.50\n2        3           1          3        15   2.00\n3        4           3          4        20   0.25",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>OrderID</th>\n      <th>CustomerID</th>\n      <th>ProductID</th>\n      <th>Quantity</th>\n      <th>Price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>5</td>\n      <td>1.00</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>10</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>15</td>\n      <td>2.00</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>3</td>\n      <td>4</td>\n      <td>20</td>\n      <td>0.25</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1631dc0bd7a86508",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Once the data is normalized, we can establish relationships between the tables using foreign keys.\n",
    "\n",
    "We can create views to get the same information as the original table by joining the normalized tables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab97c089009cd41",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### [Section 4 – Value-at-Risk]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afd6c1774223bef",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "8. [40 points] You are mandated to construct a portfolio of equities (S&P 500 and S&P/TSX indices), S&P GSCI Index, Gold, and US 10Y Treasuries and CA 10Y Treasuries. The asset mix is not determined here. (1) Keep in mind that you are a Canadian investor and you need to make up your decision on currency exposure. Explain how you arrive at your currency exposure decision and how you would implement your target currency exposure. (2) Build a process to calculate the 1-day Value-at-Risk (VaR) of the portfolio for any given asset mix. For Treasuries, you can use a simple linear approximation to calculate PnLs. (3) In your VaR calculation process, decompose VaR by the products, i.e., calculate the contributional VaR’s from the products, as well as the incremental VaR’s. Download market data from Yahoo Finance or any other reputable vendors. This can be done in code of your preferred programming language(s) or in Excel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa51a3fad723398",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1) Currency Exposure Decision:\n",
    "\n",
    "The currency exposure decision depends on the economic outlook. The current outlook indicates that the US economy is stronger than the Canadian economy.\n",
    "Canada is expected to have rate cut sooner than the US, which could lead to a weaker Canadian dollar.\n",
    "\n",
    "Based on the above, I would consider having a higher exposure to the US dollar compared to the Canadian dollar.\n",
    "To implement, I can invest more in US assets and do not hedge the currency exposure on commodities like Gold and S&P GSCI Index.\n",
    "FX exposure tilts can be achieved by using currency futures or forwards to hedge the currency exposure.\n",
    "To get target currency exposure, I can first calcuate the currency exposure without hedging and then use currency futures or forwards to adjust the exposure to the desired level.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c799f07027e44c9e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "2) VaR Calculation Process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "bbecb33053366009",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T19:50:50.896472Z",
     "start_time": "2024-04-13T19:50:50.851046Z"
    }
   },
   "outputs": [],
   "source": [
    "# Returns calculation\n",
    "Q8_DATA = DATA[(DATA.index >= '2019-12-31') & (DATA.index <= '2023-12-31')].copy()\n",
    "Q8_DATA['US10Y_%'] = Q8_DATA['US10Y_%'].diff() / 100\n",
    "Q8_DATA['CA10Y_%'] = Q8_DATA['CA10Y_%'].diff() / 100\n",
    "Q8_DATA[['SPX_US$', 'TSX_CAD$', 'SPGSCI_USD$', 'Gold_USD$', 'USDCAD_CAD$']] = Q8_DATA[\n",
    "    ['SPX_US$', 'TSX_CAD$', 'SPGSCI_USD$', 'Gold_USD$', 'USDCAD_CAD$']].pct_change()\n",
    "Q8_DATA.dropna(inplace=True)\n",
    "# Calculate price return for Treasuries assume constant duration of 8\n",
    "modified_duration = 8\n",
    "Q8_DATA['US10Y_%'] = -Q8_DATA['US10Y_%'] * modified_duration\n",
    "Q8_DATA['CA10Y_%'] = -Q8_DATA['CA10Y_%'] * modified_duration\n",
    "\n",
    "# hedge ratio to adjust portfolio level currency exposure to target\n",
    "hege_ratio = 0.3\n",
    "\n",
    "# Convert to hedged CAD returns based on hege_ratio. Hedging only on principal amount.\n",
    "Q8_DATA['US10Y_CAD%'] = Q8_DATA['US10Y_%'] + Q8_DATA['USDCAD_CAD$'] * (1 - hege_ratio)\n",
    "Q8_DATA['SPX_CAD$'] = Q8_DATA['SPX_US$'] + Q8_DATA['USDCAD_CAD$'] * (1 - hege_ratio)\n",
    "Q8_DATA['SPGSCI_CAD$'] = Q8_DATA['SPGSCI_USD$'] + Q8_DATA['USDCAD_CAD$'] * (1 - hege_ratio)\n",
    "Q8_DATA['Gold_CAD$'] = Q8_DATA['Gold_USD$'] + Q8_DATA['USDCAD_CAD$'] * (1 - hege_ratio)\n",
    "\n",
    "Q8_RETURNS = Q8_DATA[['SPX_CAD$', 'TSX_CAD$', 'SPGSCI_CAD$', 'Gold_CAD$', 'US10Y_CAD%', 'CA10Y_%']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "66b560b6f7745208",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T19:49:02.631696Z",
     "start_time": "2024-04-13T19:49:02.608337Z"
    }
   },
   "outputs": [],
   "source": [
    "def cal_historical_var(pf_returns, pf_weights, var_confidence=0.95):\n",
    "    # Calculate portfolio returns\n",
    "    pf_returns = pf_returns.dot(pf_weights)\n",
    "\n",
    "    # Calculate VaR\n",
    "    var = np.percentile(pf_returns, 100 * (1 - var_confidence))\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7b0e2a2f3eac4c19",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T19:49:02.768388Z",
     "start_time": "2024-04-13T19:49:02.750381Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "-0.012388939869001482"
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PF_WEIGHTS = np.array([0.2, 0.2, 0.2, 0.2, 0.1, 0.1])\n",
    "VAR_CONFIDENCE = 0.95\n",
    "\n",
    "HISTORICAL_VAR = cal_historical_var(Q8_RETURNS, PF_WEIGHTS, VAR_CONFIDENCE)\n",
    "HISTORICAL_VAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "eb4dc39bd9e0e7c4",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T19:49:02.916611Z",
     "start_time": "2024-04-13T19:49:02.908965Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate contribution to VaR\n",
    "def cal_contribution_to_var(asset_returns, pf_weights, var):\n",
    "    # Calculate portfolio returns\n",
    "    pf_returns = asset_returns.dot(pf_weights)\n",
    "\n",
    "    portfolio_std = pf_returns.std()\n",
    "    marginal_VaR = asset_returns.cov().dot(pf_weights) / portfolio_std\n",
    "    contributional_VaR = marginal_VaR * var / portfolio_std\n",
    "    contributional_VaR = contributional_VaR / contributional_VaR.sum() * var\n",
    "\n",
    "    return contributional_VaR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1d39fbb8dae12a58",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T19:49:03.549999Z",
     "start_time": "2024-04-13T19:49:03.543714Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "SPX_CAD$      -0.002436\nTSX_CAD$      -0.003993\nSPGSCI_CAD$   -0.004159\nGold_CAD$     -0.002373\nUS10Y_CAD%     0.000461\nCA10Y_%        0.000111\ndtype: float64"
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONTRIBUTIONAL_VAR = cal_contribution_to_var(Q8_RETURNS, PF_WEIGHTS, HISTORICAL_VAR)\n",
    "CONTRIBUTIONAL_VAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ba0ed26eca5c5fc1",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T19:49:04.171490Z",
     "start_time": "2024-04-13T19:49:04.169290Z"
    }
   },
   "outputs": [],
   "source": [
    "def cal_incremental_var(asset_returns, pf_weights, var_confidence=0.95):\n",
    "    incremental_VaR = np.zeros(len(pf_weights))\n",
    "    pf_var = cal_historical_var(asset_returns, pf_weights, var_confidence=var_confidence)\n",
    "    for i in range(len(pf_weights)):\n",
    "        new_weights = pf_weights.copy()\n",
    "        new_weights[i] = 0\n",
    "        new_weights = new_weights / new_weights.sum()\n",
    "        incremental_VaR[i] = pf_var - cal_historical_var(asset_returns, new_weights, var_confidence=var_confidence)\n",
    "    df = pd.DataFrame({'Asset': asset_returns.columns, 'Incremental VaR': incremental_VaR})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2ddb3f96326cd9b2",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T19:49:04.960311Z",
     "start_time": "2024-04-13T19:49:04.870674Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "         Asset  Incremental VaR\n0     SPX_CAD$         0.000864\n1     TSX_CAD$        -0.000831\n2  SPGSCI_CAD$        -0.001273\n3    Gold_CAD$         0.000504\n4   US10Y_CAD%         0.001891\n5      CA10Y_%         0.001461",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Asset</th>\n      <th>Incremental VaR</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>SPX_CAD$</td>\n      <td>0.000864</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>TSX_CAD$</td>\n      <td>-0.000831</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>SPGSCI_CAD$</td>\n      <td>-0.001273</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Gold_CAD$</td>\n      <td>0.000504</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>US10Y_CAD%</td>\n      <td>0.001891</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>CA10Y_%</td>\n      <td>0.001461</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INCREMENTAL_VAR = cal_incremental_var(Q8_RETURNS, PF_WEIGHTS, VAR_CONFIDENCE)\n",
    "INCREMENTAL_VAR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4127600dc622aa",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "9. [10 points] Following the setup in Q8 above, how would you calculate the 10-day VaR? What about 1-year VaR? Here, you don’t have to do the actual work but you are expected to put down your thought process and explain your assumptions and/or new assumptions and/or breaking assumptions, etc. On the other hand, you are more than welcome to work it out for bonus points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b8d1928fb1fdfd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "To calculate 10-day VaR, we can first calculate the rolling non-overlapping 10-day returns for each asset in the portfolio. Then, we can calculate the portfolio returns for each 10-day period and calculate the VaR based on these returns.\n",
    "\n",
    "To calculate 1-year VaR, we can calculate to rolling overlapping 1-year returns for each asset in the portfolio due to sample size. Then we need to adjust auto-correlation in the returns to avoid overestimation of VaR. Finally, we can calculate the portfolio returns for each 1-year period and calculate the VaR based on these returns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dae330d0638a86",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "10. [5 points] Say, you are given an estimate of 3-month VaR of $3.6B. How would you estimate the 1-year VaR without additional details? State your assumption(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751f30552fe3b34f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1-year VaR = 3-month VaR * sqrt(4) = 3.6B * 2 = 7.2B\n",
    "\n",
    "Assumptions:\n",
    "- returns are normally distributed\n",
    "- constant volatility over the time period\n",
    "- returns are independent and identically distributed (i.i.d.) over time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13794fa0d4116410",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### [Section 5 – Quantitative Finance]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0e76b2bf89eed3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "11. [5 points] How much would you pay for a call option on a single name equity with inﬁnity volatility? Explain your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d60ab7c9ea41fe",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Current price of the underlying less pv of strike price.\n",
    "\n",
    "N(d1) to 1 and N(d2) to 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8091c8c1e5d1b5a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "12. [5 points] How would you estimate the volatility of S&P 500 index? Based on the historical data from Q2 above, what’s your estimate? Explain your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "805f92c0a9f26bb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T17:55:49.041019Z",
     "start_time": "2024-04-13T17:55:48.997702Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23391149064378153"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_Q12 = DATA[(DATA.index >= '2019-12-31') & (DATA.index <= '2023-12-31')][['SPX_US$']].copy()\n",
    "DATA_Q12['SPX_US$_logDiff'] = np.log(DATA_Q12['SPX_US$'] / DATA_Q12['SPX_US$'].shift(1))\n",
    "daily_volatility = DATA_Q12['SPX_US$_logDiff'].std()\n",
    "\n",
    "# There are approximately 252 trading days in a year\n",
    "annualized_volatility = daily_volatility * np.sqrt(252)\n",
    "annualized_volatility"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Calculate daily volatility then scale it to annualized volatility."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "409b6549a4746d0a"
  },
  {
   "cell_type": "markdown",
   "id": "39f540405dd4ab5a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "13. [5 points] Estimate the default probability of a credit name quoted with a 6M CDS spread of 50 basis points assuming 50% recovery rate. Explain your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b160037b7c59f08",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "cds spread  = (1 - recovery rate) * hazard rate\n",
    "\n",
    "hazard rate is the default probability per year\n",
    "\n",
    "hazard rate = cds spread / (1 - recovery rate) = 0.5% / (1 - 0.5) = 1%\n",
    "\n",
    "6M default probability = 1% * 0.5 = 0.5%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67940a4e28ee284b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "14. [15 points] What would the delta of a put option on ETF SPY move if SPY moves up? What purpose would a put option on SPY serve in a portfolio like OTPP? Is there a way to reduce the premium cost while not losing the purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7026fef59a497ea4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- Delta of put options is always negative. The put option will be less ITM or more OTM as the price of the underlying increase. The delta of the put option will increase as the price of the underlying increases.\n",
    "\n",
    "- Provide downside protection. Strategies to reduce premium: put spread and collar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f13dded02cb9bd6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "15. [10 points] A colleague of yours provided an estimate of risk change of 5.8B risk off for OTPP’s total portfolio upon a transaction where a 1.7B credit bond exposure is sold and a 1.7B equity exposure is bought for tactical allocation purposes. Do you think this is a sensible estimate? How would you suggest validating the result? Explain your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Most likely wrong. credit bond is less volatile than equity in general unless it is a high yield bond with very high default risk. Such bond is less likely to be used in tactical allocation. Should be risk on rather than risk off."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c24ee319ed2096c1"
  },
  {
   "cell_type": "markdown",
   "id": "598e37e6f209c57f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### [Section 6 – Artiﬁcial Intelligence]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9ff34abd6bed81",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "16. [10 points] Describe an approach for an organization such as OTPP or your current workplace to utilize ChatGPT or other LLM models for extracting and understanding internal organizational data in the form of PDFs, Excel Tables and Database Tables, in terms of speciﬁc techniques/implementation? Enumerate key methods for utilizing LLM(s) for such purposes, and point out available models and their pros and cons. This is a bonus question and candidates are not required to answer it."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "store the data in vectorized form. LLM to find the answer from the data first rather than from trained answers."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3bd262ae9007446c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Github: https://github.com/krocellx/case"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91cb44905d7136d7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c4b62f2c48f666a8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
